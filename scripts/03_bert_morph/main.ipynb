{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run main.py\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "!mkdir -p {DATA_DIR} {BERT_DIR} {MODEL_DIR}\n",
    "s3 = S3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not exists(NEWS):\n",
    "    s3.download(S3_NEWS, NEWS)\n",
    "    s3.download(S3_WIKI, WIKI)\n",
    "    s3.download(S3_FICTION, FICTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not exists(BERT_VOCAB):\n",
    "    s3.download(S3_BERT_VOCAB, BERT_VOCAB)\n",
    "    s3.download(S3_BERT_EMB, BERT_EMB)\n",
    "    s3.download(S3_BERT_ENCODER, BERT_ENCODER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = list(load_lines(BERT_VOCAB))\n",
    "words_vocab = BERTVocab(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markups = {}\n",
    "for path, name in [(NEWS, TEST), (FICTION, TRAIN)]:\n",
    "    lines = load_gz_lines(path)\n",
    "    items = parse_jl(lines)\n",
    "    items = log_progress(items, desc=path)\n",
    "    records = []\n",
    "    for item in items:\n",
    "        record = MorphMarkup.from_json(item)\n",
    "        records.append(record)\n",
    "    markups[name] = records\n",
    "\n",
    "tags = set()\n",
    "for name in [TEST, TRAIN]:\n",
    "    for markup in markups[name]:\n",
    "        for token in markup.tokens:\n",
    "            tags.add(token.tag)\n",
    "            \n",
    "tags = [PAD] + sorted(tags)\n",
    "tags_vocab = TagsVocab(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(SEED)\n",
    "seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = RuBERTConfig()\n",
    "emb = BERTEmbedding(\n",
    "    config.vocab_size, config.seq_len, config.emb_dim,\n",
    "    config.dropout, config.norm_eps\n",
    ")\n",
    "encoder = BERTEncoder(\n",
    "    config.layers_num, config.emb_dim, config.heads_num, config.hidden_dim,\n",
    "    config.dropout, config.norm_eps\n",
    ")\n",
    "morph = BERTMorphHead(config.emb_dim, len(tags_vocab))\n",
    "model = BERTMorph(emb, encoder, morph)\n",
    "\n",
    "for param in emb.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "load_model(model.emb, BERT_EMB)\n",
    "load_model(model.encoder, BERT_ENCODER)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "criterion = masked_flatten_cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode = BERTMorphEncoder(\n",
    "    words_vocab, tags_vocab,\n",
    "    seq_len=128,\n",
    "    batch_size=32,\n",
    "    shuffle_size=10000\n",
    ")\n",
    "\n",
    "batches = {}\n",
    "for name in [TEST, TRAIN]:\n",
    "    batches[name] = [_.to(DEVICE) for _ in encode(markups[name])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "board = Board(BOARD_NAME, RUNS_DIR)\n",
    "boards = {\n",
    "    TRAIN: board.section(TRAIN_BOARD),\n",
    "    TEST: board.section(TEST_BOARD),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam([\n",
    "    dict(params=encoder.parameters(), lr=BERT_LR),\n",
    "    dict(params=morph.parameters(), lr=LR),\n",
    "])\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, LR_GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meters = {\n",
    "    TRAIN: MorphScoreMeter(),\n",
    "    TEST: MorphScoreMeter()\n",
    "}\n",
    "\n",
    "for epoch in log_progress(range(EPOCHS)):\n",
    "    model.train()\n",
    "    for batch in log_progress(batches[TRAIN], leave=False):\n",
    "        optimizer.zero_grad()\n",
    "        batch = process_batch(model, criterion, batch)\n",
    "        batch.loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        score = score_morph_batch(batch)\n",
    "        meters[TRAIN].add(score)\n",
    "\n",
    "    meters[TRAIN].write(boards[TRAIN])\n",
    "    meters[TRAIN].reset()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in log_progress(batches[TEST], leave=False, desc=TEST):\n",
    "            batch = process_morph_batch(model, criterion, batch)\n",
    "            score = score_batch(batch)\n",
    "            meters[TEST].add(score)\n",
    "        meters[TEST].write(boards[TEST])\n",
    "        meters[TEST].reset()\n",
    "    \n",
    "    scheduler.step()\n",
    "    board.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_model(model.encoder, MODEL_ENCODER)\n",
    "dump_model(model.morph, MODEL_MORPH)\n",
    "dump_lines(tags_vocab.items, TAGS_VOCAB)\n",
    "        \n",
    "s3.upload(MODEL_ENCODER, S3_MODEL_ENCODER)\n",
    "s3.upload(MODEL_MORPH, S3_MODEL_MORPH)\n",
    "s3.upload(TAGS_VOCAB, S3_TAGS_VOCAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
