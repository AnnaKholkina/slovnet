{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%run main.py\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "!mkdir -p {DATA_DIR} {RUBER_DIR} {MODEL_DIR}\n",
    "s3 = S3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not exists(NE5):\n",
    "    s3.download(S3_NE5, NE5)\n",
    "    s3.download(S3_BSNLP, BSNLP)\n",
    "    s3.download(S3_FACTRU, FACTRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not exists(RUBERT_VOCAB):\n",
    "    s3.download(S3_RUBERT_VOCAB, RUBERT_VOCAB)\n",
    "    s3.download(S3_RUBERT_EMB, RUBERT_EMB)\n",
    "    s3.download(S3_RUBERT_ENCODER, RUBERT_ENCODER)\n",
    "    s3.download(S3_RUBERT_NER, RUBERT_NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = list(load_lines(RUBERT_VOCAB))\n",
    "words_vocab = BERTVocab(items)\n",
    "tags_vocab = BIOTagsVocab([PER, LOC, ORG])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = CUDA0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BERTConfig(\n",
    "    vocab_size=50106,\n",
    "    seq_len=512,\n",
    "    emb_dim=768,\n",
    "    layers_num=12,\n",
    "    heads_num=12,\n",
    "    hidden_dim=3072,\n",
    "    dropout=0.1,\n",
    "    norm_eps=1e-12\n",
    ")\n",
    "emb = BERTEmbedding(\n",
    "    config.vocab_size, config.seq_len, config.emb_dim,\n",
    "    config.dropout, config.norm_eps\n",
    ")\n",
    "emb.position.requires_grad = False  # fix pos emb to train on short seqs\n",
    "encoder = BERTEncoder(\n",
    "    config.layers_num, config.emb_dim, config.heads_num, config.hidden_dim,\n",
    "    config.dropout, config.norm_eps\n",
    ")\n",
    "ner = BERTNERHead(config.emb_dim, config.vocab_size)\n",
    "model = BERTNER(emb, encoder, ner)\n",
    "\n",
    "load_model(model.emb, RUBERT_EMB)\n",
    "load_model(model.encoder, RUBERT_ENCODER)\n",
    "load_model(model.ner, RUBERT_NER)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = CRF(len(tags_vocab))\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode = BERTNEREncoder(\n",
    "    words_vocab, tags_vocab,\n",
    "    seq_len=128,\n",
    "    batch_size=32,\n",
    "    shuffle_size=10000\n",
    ")\n",
    "\n",
    "lines = (\n",
    "    line\n",
    "    for path in [NE5, BSNLP, FACTRU]\n",
    "    for line in load_lines(path)\n",
    ")\n",
    "items = parse_jl(lines)\n",
    "markups = from_jsons(items, SpanMarkup)\n",
    "batches = list(encode(markups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "board = Board(BOARD_NAME, RUNS_DIR)\n",
    "train_board = board.section(TRAIN_BOARD)\n",
    "test_board = board.section(TEST_BOARD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, 0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meters = {\n",
    "    TRAIN: NERScoreMeter(),\n",
    "    TEST: NERScoreMeter(),\n",
    "    VALID: NERScoreMeter()\n",
    "}\n",
    "\n",
    "for epoch in log_progress(range(5)):\n",
    "    model.train()\n",
    "    for batch in log_progress(batches[TRAIN], leave=False):\n",
    "        optimizer.zero_grad()\n",
    "        batch = process_batch(model, criterion, batch)\n",
    "        batch.loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        score = BatchScore(batch.loss)\n",
    "        meters[TRAIN].add(score)\n",
    "\n",
    "    meters[TRAIN].write(boards[TRAIN])\n",
    "    meters[TRAIN].reset()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for name in [TEST, VALID]:\n",
    "            for batch in log_progress(batches[name], leave=False, desc=name):\n",
    "                batch = process_batch(model, criterion, batch)\n",
    "                batch.target = split_masked(batch.target.value, batch.target.mask)\n",
    "                batch.pred = criterion.decode(batch.pred.value, batch.pred.mask)\n",
    "                score = score_batch(batch, tags_vocab)\n",
    "                meters[name].add(score)\n",
    "\n",
    "            meters[name].write(boards[name])\n",
    "            meters[name].reset()\n",
    "    \n",
    "    scheduler.step()\n",
    "    board.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
