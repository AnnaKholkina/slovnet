{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join as join_path\n",
    "\n",
    "from tqdm import tqdm_notebook as log_progress\n",
    "import pandas as pd\n",
    "\n",
    "from nerus.load import load_raw\n",
    "from nerus.const import (\n",
    "    NERUS_,\n",
    "    DUMPS_DIR as NERUS_DIR, RAW, JSONL, GZ,\n",
    "    FACTRU, NE5, GAREEV, BSNLP,\n",
    "    DEEPPAVLOV, DEEPPAVLOV_BERT, PULLENTI, TEXTERRA, TOMITA, NATASHA, MITIE\n",
    ")\n",
    "\n",
    "from navec import Navec\n",
    "\n",
    "from slovnet.record import Record\n",
    "from slovnet.bio import PER, LOC, ORG\n",
    "from slovnet.tokenizer import Tokenizer\n",
    "from slovnet.markup import SpanMarkup\n",
    "from slovnet.eval import eval_markups, avg_markup_scores\n",
    "from slovnet import NERTagger\n",
    "\n",
    "\n",
    "NERUS_DATASETS = [FACTRU, GAREEV, NE5, BSNLP]\n",
    "NERUS_ANNOTATORS = [DEEPPAVLOV_BERT, DEEPPAVLOV, PULLENTI, TEXTERRA, TOMITA, MITIE]\n",
    "\n",
    "NAVEC_DIR = join_path('..', 'navec', 'data', 'models', 'navec')\n",
    "NAVEC_MODEL = 'navec_news_v1_1B_250K_300d_100q.tar'\n",
    "\n",
    "SLOVNET = 'slovnet'\n",
    "SLOVNET_DIR = join_path('data', 'models')\n",
    "SLOVNET_MODEL = 'slovnet_ner_v1.tar'\n",
    "\n",
    "DATASETS = NERUS_DATASETS\n",
    "ANNOTATORS = [SLOVNET] + NERUS_ANNOTATORS\n",
    "TYPES = [PER, LOC, ORG]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NERUS = {}\n",
    "for dataset in log_progress(NERUS_DATASETS):\n",
    "    path = join_path(NERUS_DIR, NERUS_ + dataset + RAW + JSONL + GZ)\n",
    "    NERUS[dataset] = list(load_raw(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adapt(markups):\n",
    "    for markup in markups:\n",
    "        markup = markup.adapted\n",
    "        yield SpanMarkup(markup.text, markup.spans)\n",
    "\n",
    "\n",
    "MARKUPS = {}\n",
    "for dataset in log_progress(NERUS_DATASETS):\n",
    "    records = NERUS[dataset]\n",
    "    \n",
    "    # target\n",
    "    markups = [_.source for _ in records]\n",
    "    MARKUPS[dataset, dataset] = list(adapt(markups))\n",
    "    \n",
    "    # nerus preds\n",
    "    for annotator in log_progress(NERUS_ANNOTATORS, leave=False):\n",
    "        markups = [_.find(annotator) for _ in records]\n",
    "        MARKUPS[dataset, annotator] = list(adapt(markups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "path = join_path(NAVEC_DIR, NAVEC_MODEL)\n",
    "navec = Navec.load(path)\n",
    "\n",
    "path = join_path(SLOVNET_DIR, SLOVNET_MODEL)\n",
    "slovnet_ner = NERTagger.load(path, navec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in log_progress(NERUS_DATASETS):\n",
    "    records = NERUS[dataset]\n",
    "    markups = [\n",
    "        slovnet_ner(_.text)\n",
    "        for _ in log_progress(records, leave=False)\n",
    "    ]\n",
    "    MARKUPS[dataset, SLOVNET] = markups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER = Tokenizer()\n",
    "TAG_MARKUPS = {}\n",
    "for key in log_progress(MARKUPS):\n",
    "    markups = MARKUPS[key]\n",
    "    TAG_MARKUPS[key] = [_.to_tag(TOKENIZER) for _ in markups]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCORES = {}\n",
    "for dataset in log_progress(DATASETS):\n",
    "    for annotator in log_progress(ANNOTATORS, leave=False):\n",
    "        targets = TAG_MARKUPS[dataset, dataset]\n",
    "        preds = TAG_MARKUPS[dataset, annotator]\n",
    "        score = avg_markup_scores(eval_markups(preds, targets, TYPES))\n",
    "        for type in TYPES:\n",
    "            SCORES[dataset, annotator, type] = score.get(type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_table(scores, datasets=DATASETS, annotators=ANNOTATORS, types=TYPES):\n",
    "    data = []\n",
    "    for dataset in datasets:\n",
    "        for annotator in annotators:\n",
    "            for type in types:\n",
    "                score = scores[dataset, annotator, type]\n",
    "                data.append([dataset, annotator, type, score])\n",
    "    table = pd.DataFrame(data, columns=['dataset', 'annotator', 'type', 'score'])\n",
    "    table = table.set_index(['dataset', 'annotator', 'type']).unstack(['dataset', 'type'])\n",
    "\n",
    "    table.columns = table.columns.droplevel()\n",
    "    table.index.name = None\n",
    "\n",
    "    columns = [\n",
    "        (dataset, type)\n",
    "        for dataset in datasets\n",
    "        for type in types\n",
    "    ]\n",
    "    table = table.reindex(index=annotators, columns=columns)\n",
    "\n",
    "    return table\n",
    "\n",
    "\n",
    "def format_score(value):\n",
    "    if not value:\n",
    "        return '-'\n",
    "    return '{0:02d}'.format(int(value * 100))\n",
    "\n",
    "\n",
    "def format_scores(scores):\n",
    "    return '{prec}/{recall}/{f1}'.format(\n",
    "        prec=format_score(scores.prec.value),\n",
    "        recall=format_score(scores.recall.value),\n",
    "        f1=format_score(scores.f1)\n",
    "    )\n",
    "\n",
    "\n",
    "def format_report(table):\n",
    "    output = pd.DataFrame()\n",
    "    for column in table.columns:\n",
    "        output[column] = table[column].map(format_scores)\n",
    "\n",
    "    output.columns = pd.MultiIndex.from_tuples(output.columns)\n",
    "    output.columns.names = [None, 'prec/recall/f1,%']\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "table = report_table(SCORES)\n",
    "output = format_report(table)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_github_column(column, top=3):\n",
    "    column = [\n",
    "        (_.f1 if _ else None)\n",
    "        for _ in column\n",
    "    ]\n",
    "\n",
    "    selection = None\n",
    "    values = list(filter(None, column))\n",
    "    selection = sorted(values)[-top:]\n",
    "\n",
    "    for value in column:\n",
    "        cell = ''\n",
    "        if value:\n",
    "            cell = '%.3f' % value\n",
    "        if value in selection:\n",
    "            cell = '<b>%s</b>' % cell\n",
    "        yield cell\n",
    "\n",
    "\n",
    "def format_github_report(table):\n",
    "    output = pd.DataFrame()\n",
    "    for column in table.columns:\n",
    "        dataset, type = column\n",
    "        if dataset == GAREEV and type == LOC:\n",
    "            continue\n",
    "        output[column] = list(format_github_column(table[column]))\n",
    "\n",
    "    output.index = table.index\n",
    "    output.columns = pd.MultiIndex.from_tuples(output.columns)\n",
    "    output.columns.names = [None, 'f1']\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "output = format_github_report(table)\n",
    "html = output.to_html(escape=False)\n",
    "html = html.replace('border=\"1\"', 'border=\"0\"')\n",
    "display(output)\n",
    "print(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see nerus/bench.ipynb\n",
    "\n",
    "class Stats(Record):\n",
    "    __attributes__ = ['annotator', 'init', 'disk', 'ram', 'speed']\n",
    "    \n",
    "    def __init__(self, annotator, init=None, disk=None, ram=None, speed=None):\n",
    "        self.annotator = annotator\n",
    "        self.init = init\n",
    "        self.disk = disk\n",
    "        self.ram = ram\n",
    "        self.speed = speed\n",
    "        \n",
    "\n",
    "class GPUStats(Stats):\n",
    "    pass\n",
    "\n",
    "\n",
    "KB = 1024\n",
    "MB = 1024 * KB\n",
    "GB = 1024 * MB\n",
    "        \n",
    "\n",
    "STATS = [\n",
    "    # GTX 1080 Ti\n",
    "    GPUStats(\n",
    "        DEEPPAVLOV,\n",
    "        init=5.9,\n",
    "        disk=1 * GB,  # 1GB emb + 5MB model\n",
    "        ram=3 * GB,\n",
    "        speed=24.31,  # 1.95 / 7 cores on CPU\n",
    "    ),\n",
    "    GPUStats(\n",
    "        DEEPPAVLOV_BERT,\n",
    "        init=34.5,\n",
    "        disk=2 * GB,\n",
    "        ram=6 * GB,\n",
    "        speed=13.13,  # 17.71 / 3 cores on CPU\n",
    "    ),\n",
    "    \n",
    "    # 16 CPUs\n",
    "    Stats(\n",
    "        PULLENTI,\n",
    "        init=2.85,\n",
    "        disk=16 * MB,\n",
    "        ram=253 * MB,\n",
    "        speed=6.05\n",
    "    ),\n",
    "    Stats(\n",
    "        TEXTERRA,\n",
    "        init=47.6,\n",
    "        disk=193 * MB,\n",
    "        ram=3.3 * GB,  # leaks\n",
    "        speed=20.16 / 5  # utils ~5 cores\n",
    "    ),\n",
    "    Stats(\n",
    "        TOMITA,\n",
    "        init=2.03,\n",
    "        disk=64 * MB,\n",
    "        ram=63 * MB,\n",
    "        speed=29.8,\n",
    "    ),\n",
    "    Stats(\n",
    "        NATASHA,\n",
    "        init=2.05,\n",
    "        disk=700 * KB,\n",
    "        ram=160 * MB,\n",
    "        speed=8.8,\n",
    "    ),\n",
    "    Stats(\n",
    "        MITIE,\n",
    "        init=28.3,\n",
    "        disk=327 * MB,\n",
    "        ram=261 * MB,\n",
    "        speed=32.8,\n",
    "    ),    \n",
    "]\n",
    "\n",
    "\n",
    "STATS += [\n",
    "    Stats(\n",
    "        SLOVNET,\n",
    "        init=0.9,\n",
    "        disk=30 * MB,\n",
    "        ram=180 * MB,\n",
    "        speed=33.92\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_max(values, count=3):\n",
    "    return sorted(values)[-count:]\n",
    "\n",
    "\n",
    "def select_min(values, count=3):\n",
    "    return sorted(values)[:count]\n",
    "\n",
    "\n",
    "def slice_attr(records, attr):\n",
    "    for record in records:\n",
    "        yield getattr(record, attr)\n",
    "        \n",
    "        \n",
    "def slice_init(records): return slice_attr(records, 'init')\n",
    "def slice_disk(records): return slice_attr(records, 'disk')\n",
    "def slice_ram(records): return slice_attr(records, 'ram')\n",
    "\n",
    "\n",
    "def slice_speed(records):\n",
    "    for record in records:\n",
    "        yield record.speed, record.__class__ is GPUStats\n",
    "\n",
    "\n",
    "def highlight(column, selection, format):\n",
    "    for value in column:\n",
    "        select = value in selection\n",
    "        value = format(value)\n",
    "        if select:\n",
    "            value = '<b>%s</b>' % value\n",
    "        yield value\n",
    "\n",
    "\n",
    "def format_mb(bytes):\n",
    "    mb = bytes / MB\n",
    "    return '%0.0f' % mb\n",
    "\n",
    "\n",
    "def format_sec(secs):\n",
    "    return '%0.1f' % secs\n",
    "\n",
    "\n",
    "def format_speed(value):\n",
    "    its, gpu = value\n",
    "    value = '%0.1f' % its\n",
    "    if gpu:\n",
    "        value += ' (gpu)'\n",
    "    return value\n",
    "\n",
    "\n",
    "def format_report(stats, annotators=ANNOTATORS):\n",
    "    table = pd.DataFrame()\n",
    "\n",
    "    mapping = {_.annotator: _ for _ in stats}\n",
    "    stats = [mapping[_] for _ in annotators]\n",
    "\n",
    "    columns = [\n",
    "        [slice_init, format_sec, select_min, 'init, s'],\n",
    "        [slice_disk, format_mb, select_min, 'disk, mb'],\n",
    "        [slice_ram, format_mb, select_min, 'ram, mb'],\n",
    "        [slice_speed, format_speed, select_max, 'speed, articles/s']\n",
    "    ]\n",
    "    for slice, format, select, name in columns:\n",
    "        values = list(slice(stats))\n",
    "        selection = select(values)\n",
    "        table[name] = list(highlight(values, selection, format))\n",
    "\n",
    "    table.index = annotators\n",
    "    table.index.name = None\n",
    "    return table\n",
    "\n",
    "\n",
    "output = format_report(STATS)\n",
    "html = output.to_html(escape=False)\n",
    "html = html.replace('border=\"1\"', 'border=\"0\"')\n",
    "display(output)\n",
    "print(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
